{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a236bbb-da30-4fc2-9793-9d399cd1e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\PC\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import glob, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, array_to_img, load_img\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('TensorFlow Version ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d62c50-e784-4ee0-8e13-34323386585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a5419-3e29-47f5-a116-006311b2040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    \n",
    "    # Flips\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > .75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > .75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n",
    "    elif p_rotate > .5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n",
    "    elif p_rotate > .25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n",
    "        \n",
    "    # Pixel-level transforms\n",
    "    if p_pixel_1 >= .4:\n",
    "        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n",
    "    if p_pixel_2 >= .4:\n",
    "        image = tf.image.random_contrast(image, lower = .8, upper = 1.2)\n",
    "    if p_pixel_3 >= .4:\n",
    "        image = tf.image.random_brightness(image, max_delta = .1)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5c0ea-83d2-460d-bf75-bad87d7f7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                                          samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True,\n",
    "                                                          validation_split = 0.2,\n",
    "                                                          preprocessing_function = data_augment)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "                                        'D:/DesktopMal/malimg_dataset/train',\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = (224, 224))\n",
    "\n",
    "valid_gen = datagen.flow_from_directory(\n",
    "                                        'D:/DesktopMal/malimg_dataset/train',\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = (224, 224))\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "                                        'D:/DesktopMal/malimg_dataset/train',\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4db6a-bee3-4265-b067-2dd044fb7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_gen.class_indices\n",
    "t_name=[k for  k,v in classes.items()]\n",
    "classes = dict((v,k) for k,v in classes.items())\n",
    "# classes = [classes[k] for k in classes]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65182ca6-5abc-4d76-a368-0ae178f4f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa01774-95a0-4fee-8c7a-f7d6a8f98adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit\n",
    "vit_model = vit.vit_b16(\n",
    "        image_size = 128,\n",
    "        activation = 'softmax',\n",
    "        pretrained = True,\n",
    "        include_top = False,\n",
    "        pretrained_top = False,\n",
    "        classes = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809ab86-97d3-4a82-9378-64385770cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(25, 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1951248b-8cea-4c19-a24d-a8d9b3997c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vision_transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit-b16 (Functional)        (None, 768)               85697280  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                24608     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32)               128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 48)                1584      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48)               192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 112)               5488      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 112)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 25)                2825      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,732,105\n",
      "Trainable params: 85,731,945\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        vit_model,\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=32, activation='selu', kernel_initializer='glorot_uniform'),\n",
    "tf.keras.layers.BatchNormalization(),\n",
    "tf.keras.layers.Dense(units=48, activation='softplus', kernel_initializer='glorot_normal'),\n",
    "tf.keras.layers.BatchNormalization(),\n",
    "tf.keras.layers.Dropout(0.25),\n",
    "tf.keras.layers.Dense(units=112, activation='relu', kernel_initializer='lecun_uniform'),\n",
    "\n",
    "tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(25, 'softmax')\n",
    "    ],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6464d-c5cf-4833-91a8-dd1ef7b5a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                                                factor = 0.2,\n",
    "                                                patience = 2,\n",
    "                                                verbose = 1,\n",
    "                                                min_delta = 1e-4,\n",
    "                                                min_lr = 1e-6,\n",
    "                                                mode = 'max')\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                min_delta = 1e-4,\n",
    "                                                patience = 5,\n",
    "                                                mode = 'max',\n",
    "                                                restore_best_weights = True,\n",
    "                                                verbose = 1)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
    "                                                 monitor = 'val_accuracy', \n",
    "                                                 verbose = 1, \n",
    "                                                 save_best_only = True,\n",
    "                                                 save_weights_only = True,\n",
    "                                                mode = 'max')\n",
    "\n",
    "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
    "\n",
    "model.fit(X_train,y_train,\n",
    "         #steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "         #validation_data = valid_gen,\n",
    "         #validation_steps = STEP_SIZE_VALID,\n",
    "         epochs = EPOCHS, validation_split=0.1)\n",
    "         #callbacks = callbacks)\n",
    "\n",
    "#model.fit(x = train_gen,validation_data = valid_gen)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a796fc-e9ad-4bee-8590-895e430491c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "704/704 [==============================] - 244s 254ms/step - loss: 1.7720 - accuracy: 0.4823 - val_loss: 0.4096 - val_accuracy: 0.8572\n",
      "Epoch 2/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.4655 - accuracy: 0.8502 - val_loss: 0.2581 - val_accuracy: 0.9056\n",
      "Epoch 3/50\n",
      "704/704 [==============================] - 176s 250ms/step - loss: 0.2969 - accuracy: 0.9000 - val_loss: 0.2080 - val_accuracy: 0.9356\n",
      "Epoch 4/50\n",
      "704/704 [==============================] - 176s 250ms/step - loss: 0.2069 - accuracy: 0.9362 - val_loss: 0.1236 - val_accuracy: 0.9476\n",
      "Epoch 5/50\n",
      "704/704 [==============================] - 176s 250ms/step - loss: 0.1548 - accuracy: 0.9532 - val_loss: 0.0964 - val_accuracy: 0.9660\n",
      "Epoch 6/50\n",
      "704/704 [==============================] - 176s 250ms/step - loss: 0.1226 - accuracy: 0.9646 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 7/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0968 - accuracy: 0.9724 - val_loss: 0.0605 - val_accuracy: 0.9820\n",
      "Epoch 8/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0854 - accuracy: 0.9750 - val_loss: 0.0530 - val_accuracy: 0.9824\n",
      "Epoch 9/50\n",
      "704/704 [==============================] - 178s 252ms/step - loss: 0.0672 - accuracy: 0.9796 - val_loss: 0.0393 - val_accuracy: 0.9856\n",
      "Epoch 10/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0581 - accuracy: 0.9842 - val_loss: 0.0305 - val_accuracy: 0.9896\n",
      "Epoch 11/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0464 - accuracy: 0.9881 - val_loss: 0.0315 - val_accuracy: 0.9924\n",
      "Epoch 12/50\n",
      "704/704 [==============================] - 178s 252ms/step - loss: 0.0440 - accuracy: 0.9891 - val_loss: 0.0314 - val_accuracy: 0.9880\n",
      "Epoch 13/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0373 - accuracy: 0.9904 - val_loss: 0.0211 - val_accuracy: 0.9940\n",
      "Epoch 14/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0377 - accuracy: 0.9906 - val_loss: 0.0399 - val_accuracy: 0.9884\n",
      "Epoch 15/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0246 - accuracy: 0.9941 - val_loss: 0.0188 - val_accuracy: 0.9944\n",
      "Epoch 16/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0252 - accuracy: 0.9937 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
      "Epoch 17/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0229 - accuracy: 0.9945 - val_loss: 0.0163 - val_accuracy: 0.9952\n",
      "Epoch 18/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0223 - accuracy: 0.9949 - val_loss: 0.0207 - val_accuracy: 0.9948\n",
      "Epoch 19/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0153 - accuracy: 0.9969 - val_loss: 0.0189 - val_accuracy: 0.9944\n",
      "Epoch 20/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0172 - accuracy: 0.9960 - val_loss: 0.0201 - val_accuracy: 0.9964\n",
      "Epoch 21/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0123 - accuracy: 0.9976 - val_loss: 0.0424 - val_accuracy: 0.9908\n",
      "Epoch 22/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0123 - accuracy: 0.9976 - val_loss: 0.0145 - val_accuracy: 0.9964\n",
      "Epoch 23/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0147 - accuracy: 0.9973 - val_loss: 0.0125 - val_accuracy: 0.9972\n",
      "Epoch 24/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0094 - accuracy: 0.9982 - val_loss: 0.0307 - val_accuracy: 0.9920\n",
      "Epoch 25/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0125 - accuracy: 0.9973 - val_loss: 0.0136 - val_accuracy: 0.9972\n",
      "Epoch 26/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0143 - accuracy: 0.9971 - val_loss: 0.0135 - val_accuracy: 0.9972\n",
      "Epoch 27/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.0134 - val_accuracy: 0.9968\n",
      "Epoch 28/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 0.0160 - val_accuracy: 0.9972\n",
      "Epoch 29/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0198 - val_accuracy: 0.9968\n",
      "Epoch 30/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0201 - accuracy: 0.9948 - val_loss: 0.0224 - val_accuracy: 0.9948\n",
      "Epoch 31/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0091 - accuracy: 0.9982 - val_loss: 0.0340 - val_accuracy: 0.9924\n",
      "Epoch 32/50\n",
      "704/704 [==============================] - 178s 253ms/step - loss: 0.0149 - accuracy: 0.9967 - val_loss: 0.0150 - val_accuracy: 0.9972\n",
      "Epoch 33/50\n",
      "704/704 [==============================] - 178s 253ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 0.0132 - val_accuracy: 0.9968\n",
      "Epoch 34/50\n",
      "704/704 [==============================] - 178s 252ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.0151 - val_accuracy: 0.9972\n",
      "Epoch 35/50\n",
      "704/704 [==============================] - 178s 252ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.0134 - val_accuracy: 0.9976\n",
      "Epoch 36/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0153 - val_accuracy: 0.9964\n",
      "Epoch 37/50\n",
      "704/704 [==============================] - 178s 252ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0179 - val_accuracy: 0.9960\n",
      "Epoch 38/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0089 - accuracy: 0.9982 - val_loss: 0.0189 - val_accuracy: 0.9968\n",
      "Epoch 39/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0303 - val_accuracy: 0.9956\n",
      "Epoch 40/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0184 - val_accuracy: 0.9964\n",
      "Epoch 41/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0253 - val_accuracy: 0.9960\n",
      "Epoch 42/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.0166 - val_accuracy: 0.9972\n",
      "Epoch 43/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0171 - val_accuracy: 0.9968\n",
      "Epoch 44/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0227 - val_accuracy: 0.9960\n",
      "Epoch 45/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.0207 - val_accuracy: 0.9968\n",
      "Epoch 46/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.0206 - val_accuracy: 0.9960\n",
      "Epoch 47/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0211 - val_accuracy: 0.9968\n",
      "Epoch 48/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0197 - val_accuracy: 0.9972\n",
      "Epoch 49/50\n",
      "704/704 [==============================] - 177s 252ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0225 - val_accuracy: 0.9968\n",
      "Epoch 50/50\n",
      "704/704 [==============================] - 177s 251ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.0183 - val_accuracy: 0.9972\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.001, momentum = 0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "historyCNN_ANN = model.fit(X_train,y_train, epochs=50, validation_data=(X_test,y_test),verbose=1,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb387fab-eeb7-4697-8a1a-5b9dc5393975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5c547-747c-4a11-8cf8-14a6d43b632e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49afae-87a7-4710-bc4e-d8fd011e3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN'in train/validation loss ve accuraccy'lerindeki deฤiลimi \n",
    "# gรถrmek amacฤฑyla bir fonksiyon yazฤฑyorum\n",
    "\n",
    "def plot_historyACC(history, model):\n",
    "    plt.subplots(figsize=(7,10))\n",
    "    plt.subplot(2,1,1) \n",
    "    plt.ylim(0,1)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], color='green', alpha = 0.7, label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], color='orange', alpha = 0.7, label='Validation Loss')\n",
    "    plt.legend() \n",
    "    plt.subplot(2,1,2) \n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='green', alpha = 0.7, label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', alpha = 0.7, label='Validation Accuracy')\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158c269-03a5-4111-b3dd-d8c370ce418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_historyACC(historyCNN_ANN, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019feb91-e67c-4825-9d43-a1573b8632c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02693e90-a464-481b-bf99-ee92cab65461",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0318e-3ab0-4eef-a590-a0bc471596f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[50],test_gen.classes[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5914cba-4fa4-4d2a-9e66-5a356f92ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_gen.classes, y_pred))\n",
    "print('Classification Report')\n",
    "print(classification_report(test_gen.classes, y_pred, target_names=t_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
